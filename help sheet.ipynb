{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bdbe033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/samarthpawar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/samarthpawar/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/samarthpawar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk import FreqDist\n",
    "from nltk import bigrams\n",
    "from nltk import trigrams\n",
    "from nltk import ngrams\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm import Laplace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1bb4ed",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed4be0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = \"\"\n",
    "is_lemmatization = True\n",
    "is_stemming = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1242f873",
   "metadata": {},
   "source": [
    "## Helpful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fdc86a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_small_case(text):\n",
    "  return ' '.join(re.findall(r'\\w+', text.lower()))\n",
    "\n",
    "\n",
    "token_counts = Counter(get_small_case(text_corpus))\n",
    "\n",
    "\n",
    "def P(word, N=sum(token_counts.values())):\n",
    "  \"\"\"Probability of word\"\"\"\n",
    "  return token_counts[word] / N\n",
    "\n",
    "def correction(word):\n",
    "  \"\"\"Most probable spelling correction for word\"\"\"\n",
    "  return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word):\n",
    "  \"\"\"Possible corrections for word\"\"\"\n",
    "  return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words):\n",
    "  \"\"\"The subset of 'words' that appear in the dictionary of token_counts\"\"\"\n",
    "  return set(w for w in words if w in token_counts)\n",
    "\n",
    "def edits1(word):\n",
    "  \"\"\"All edits that are one edit away from 'word'\"\"\"\n",
    "  letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "  splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "  deletes = [L + R[1:] for L, R in splits if R ]\n",
    "  transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "  replaces = [L + c + R[1:] for L, R in splits if R for c in letters ]\n",
    "  inserts = [L + c + R for L, R in splits for c in letters]\n",
    "  return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word):\n",
    "  \"\"\"All the edits are two edits away from word\"\"\"\n",
    "  return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fb6152",
   "metadata": {},
   "source": [
    "## Word Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ffa63ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_lemmatization(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ca7e9",
   "metadata": {},
   "source": [
    "## Word Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6c0f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_stemming(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9838aa",
   "metadata": {},
   "source": [
    "## Zipf's Law for given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f69a6cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_zipfs_law(tokens):\n",
    "    token_counts = Counter(tokens)\n",
    "    # Frequency of n-th most frequent word and 1/n line.\n",
    "    freqs = [count for _, count in token_counts.most_common()]\n",
    "    ranks = range(1, len(freqs) + 1)\n",
    "    plt.plot(ranks, freqs)\n",
    "    plt.plot(ranks, [freqs[0] / rank for rank in ranks])\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Rank')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Zipf\\'s Law')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7b3953",
   "metadata": {},
   "source": [
    "## Questions (Tut 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3598bdd6",
   "metadata": {},
   "source": [
    "### Question 1: Show Zipf's law using Gutenberg Corpus? (download the Gutenberg Corpus using nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a81cf8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/samarthpawar/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show Zipf's law using Gutenberg Corpus? (download the Gutenberg Corpus using nltk)\n",
    "\n",
    "# download gutenberg corpus\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "# get the words from the corpus\n",
    "words = gutenberg.words()\n",
    "if is_lemmatization:\n",
    "    words = do_lemmatization(words)\n",
    "\n",
    "if is_stemming:\n",
    "    words = do_stemming(words)\n",
    "# show the Zipf's law\n",
    "show_zipfs_law(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f336efb",
   "metadata": {},
   "source": [
    "### Question 2: Perform word tokenization on string \n",
    "\n",
    "\"Let’s go and meet Mr. Takashi, one of the prominent members of N.C.A. in Japan.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "246fc12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lets', 'go', 'and', 'meet', 'Mr', 'Takashi', 'one', 'of', 'the', 'prominent', 'members', 'of', 'NCA', 'in', 'Japan']\n"
     ]
    }
   ],
   "source": [
    "# Perform word tokenization on text string\n",
    "text = \"Let’s go and meet Mr. Takashi, one of the prominent members of N.C.A. in Japan.\"\n",
    "cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "tokens = word_tokenize(cleaned_text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518e3421",
   "metadata": {},
   "source": [
    "### Question(3) Write a custom function to perform stemming to handle \"ed\" at the end of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c30f16db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'play'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_stemming_ed(word):\n",
    "    \"\"\"Custom Stemming ending with ed, ing, s\"\"\"\n",
    "    if word.endswith('ing'):\n",
    "        return word[:-3]\n",
    "    elif word.endswith('ed'):\n",
    "        return word[:-2]\n",
    "    elif word.endswith('s'):\n",
    "        return word[:-1]\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "custom_stemming_ed('played')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1614014d",
   "metadata": {},
   "source": [
    "### Question 4: Remove emails from the given text using regular expressions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bbc49c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dave Martin\n",
      "615-555-7164\n",
      "173 Main St., Springfield RI 55924\n",
      "Charles Harris\n",
      "800-555-5669a\n",
      "969 High St., Atlantis VA 34075\n",
      "Eric Williams\n",
      "560-555-5153\n",
      "806 1st St., Faketown AK 86847\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_emails(text):\n",
    "    \"\"\"Remove emails from text\"\"\"\n",
    "    return re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "\n",
    "text = \"\"\"\n",
    "Dave Martin\n",
    "615-555-7164\n",
    "173 Main St., Springfield RI 55924\n",
    "davemartin@gmail.com\n",
    "Charles Harris\n",
    "800-555-5669a\n",
    "969 High St., Atlantis VA 34075\n",
    "charlesharris@bogusemail.com\n",
    "Eric Williams\n",
    "560-555-5153\n",
    "806 1st St., Faketown AK 86847\n",
    "laurawilliams@outlook.com\n",
    "\"\"\"\n",
    "\n",
    "print(remove_emails(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfb104a",
   "metadata": {},
   "source": [
    "# Tutorial 2 stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1f15316c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/samarthpawar/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "nltk.download('brown')\n",
    "from nltk.lm import MLE\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf938666",
   "metadata": {},
   "source": [
    "## Bigram Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24d4168d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'b'), ('b', 'c')]\n",
      "[('a', 'c'), ('c', 'd'), ('d', 'c'), ('c', 'e'), ('e', 'f')]\n"
     ]
    }
   ],
   "source": [
    "# Dummy text data containing two sentences\n",
    "text = [['a', 'b', 'c'], ['a', 'c', 'd', 'c', 'e', 'f']] # List of list format \n",
    "# Each sentence must be a list of words i.e. sentence.split() will do the job\n",
    "print(list(bigrams(text[0])))\n",
    "print(list(bigrams(text[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebea1372",
   "metadata": {},
   "source": [
    "## N-grams Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ab7a05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'c', 'd'), ('c', 'd', 'c'), ('d', 'c', 'e'), ('c', 'e', 'f')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(text[1], n=3)) # Tri-grams of second sentence "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cedcc19",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7146df0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'a', 'b', 'c', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# But since we are not able to see starting and ending of the word, hence, we will use pad_sequence\n",
    "padded_sent = list(pad_sequence(text[0], # Text to be padded\n",
    "                  pad_left=True, # Padding to the left\n",
    "                  left_pad_symbol=\"<s>\", \n",
    "                  pad_right=True, # Padding to the right \n",
    "                  right_pad_symbol=\"</s>\", \n",
    "                  n=2)) # No. of grams \n",
    "print(padded_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e592961d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', 'a'), ('a', 'b'), ('b', 'c'), ('c', '</s>')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(padded_sent, n=2)) # 2-grams \n",
    "# Now we have the starting and ending of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d14b905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '<s>', 'a', 'b', 'c', '</s>', '</s>']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for N gram model we put N-1 padding at the start and end of the sentence\n",
    "list(pad_both_ends(text[0], n=3)) # Using pad_both_ends directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c65ba0f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', '<s>', 'a'),\n",
       " ('<s>', 'a', 'c'),\n",
       " ('a', 'c', 'd'),\n",
       " ('c', 'd', 'c'),\n",
       " ('d', 'c', 'e'),\n",
       " ('c', 'e', 'f'),\n",
       " ('e', 'f', '</s>'),\n",
       " ('f', '</s>', '</s>')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(pad_both_ends(text[1], n=3), n=3)) # Using pad_both_ends directly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83ad6a",
   "metadata": {},
   "source": [
    "## All grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2736cac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>',),\n",
       " ('<s>', 'a'),\n",
       " ('a',),\n",
       " ('a', 'b'),\n",
       " ('b',),\n",
       " ('b', 'c'),\n",
       " ('c',),\n",
       " ('c', '</s>'),\n",
       " ('</s>',)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_bigrams = list(pad_both_ends(text[0], n=2))\n",
    "all_grams = list(everygrams(padded_bigrams, max_len=2))\n",
    "all_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b722493",
   "metadata": {},
   "source": [
    "### Printing the count of bigrams and trigrams from the all gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de304dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unigrams:  5\n",
      "Total number of bigrams:  4\n"
     ]
    }
   ],
   "source": [
    "def print_n_count_allgrams(all_grams):\n",
    "    # Calculating the number of bi-grmas and unigrmas in first sentence [a, b, c]\n",
    "    count = {}\n",
    "    for i in all_grams:\n",
    "        if len(i) == 1:\n",
    "            if count.get(\"unigrams\"): count[\"unigrams\"] = count.get(\"unigrams\") + 1\n",
    "            else: count[\"unigrams\"] = 1\n",
    "        else:\n",
    "            if count.get(\"bigrams\"): count[\"bigrams\"] = count.get(\"bigrams\") + 1\n",
    "            else: count[\"bigrams\"] = 1\n",
    "    print(\"Total number of unigrams: \", count.get(\"unigrams\"))\n",
    "    print(\"Total number of bigrams: \", count.get(\"bigrams\"))\n",
    "\n",
    "print_n_count_allgrams(all_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b43e816",
   "metadata": {},
   "source": [
    "## Generating Vocabulary (Flatten list of our All grams list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf287e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'a', 'b', 'c', '</s>', '<s>', 'a', 'c', 'd', 'c', 'e', 'f', '</s>']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the padded sequence as a single list of words which is nothing but our vocabulary \n",
    "list(flatten(pad_both_ends(sent, n=2) for sent in text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "488ca4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-grams for sentence 1\n",
      "[('<s>',), ('<s>', 'a'), ('a',), ('a', 'b'), ('b',), ('b', 'c'), ('c',), ('c', '</s>'), ('</s>',)]\n",
      "n-grams for sentence 2\n",
      "[('<s>',), ('<s>', 'a'), ('a',), ('a', 'c'), ('c',), ('c', 'd'), ('d',), ('d', 'c'), ('c',), ('c', 'e'), ('e',), ('e', 'f'), ('f',), ('f', '</s>'), ('</s>',)]\n"
     ]
    }
   ],
   "source": [
    "# Better way to generate vocabulary and padded sequence\n",
    "training_ngrams, padded_sentences = padded_everygram_pipeline(2, text)\n",
    "for ind, ngramlize_sent in enumerate(training_ngrams):\n",
    "  print(f\"n-grams for sentence {ind + 1}\")\n",
    "  print(list(ngramlize_sent))\n",
    "# training_ngrams provides us with all_grams for every sentence in our text \n",
    "# Padded_sentences provides is nothing but our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e23642bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'a', 'b', 'c', '</s>', '<s>', 'a', 'c', 'd', 'c', 'e', 'f', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(list(padded_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6405a4",
   "metadata": {},
   "source": [
    "## Example of making a model for real dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f55a8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Paragraphs : 500\n",
      "['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', 'ca08', 'ca09', 'ca10']\n",
      "No. of sentences in first paragraph 98.\n",
      "First sentence from first paragraph: The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n"
     ]
    }
   ],
   "source": [
    "# Brown dataset\n",
    "all_para_ids = nltk.corpus.brown.fileids()\n",
    "print(\"Total Number of Paragraphs :\", len(all_para_ids))\n",
    "print(all_para_ids[:10])\n",
    "print(f\"No. of sentences in first paragraph {len(nltk.corpus.brown.sents(all_para_ids[0]))}.\")\n",
    "print(f\"First sentence from first paragraph: {' '.join(nltk.corpus.brown.sents(all_para_ids[0])[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd7cd814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences in Brown corpus: 57340\n"
     ]
    }
   ],
   "source": [
    "# Create list of all sentences present in \"brown\" coupus\n",
    "full_brown = []\n",
    "\n",
    "for text in all_para_ids:\n",
    "    para = nltk.corpus.brown.sents(text)\n",
    "    full_brown += [list(i) for i in para]\n",
    "\n",
    "print(f\"Total number of sentences in Brown corpus: {len(full_brown)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273a7e63",
   "metadata": {},
   "source": [
    "## Training our N-gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "639a830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "model = MLE(n) # Lets train a 3-grams model, previously we set n=3\n",
    "\n",
    "# Preprocess the tokenized text for 3-grams language modelling\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, full_brown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2737aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 56060 items>\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(train_data, padded_sents)\n",
    "print(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35887391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 56060 items>\n"
     ]
    }
   ],
   "source": [
    "# n-gram model with laplace smoothing\n",
    "model = Laplace(n)\n",
    "# Preprocess the tokenized text for 3-grams language modelling\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, full_brown)\n",
    "model.fit(train_data, padded_sents)\n",
    "print(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14bf9846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n",
      "6\n",
      "1\n",
      "7.396592866642887e-05\n",
      "0.00012482836100362001\n",
      "3.567224342738915e-05\n"
     ]
    }
   ],
   "source": [
    "# Using model to retrieve counts\n",
    "print(model.counts['language']) # count(language)\n",
    "\n",
    "# count(Fulton, County)\n",
    "print(model.counts[['Fulton']]['County'])\n",
    "\n",
    "# count(Fulton, County, Grand)\n",
    "print(model.counts[['Fulton', 'County']]['Grand'])\n",
    "\n",
    "# P(language)\n",
    "print(model.score('language'))\n",
    "\n",
    "# P(County | Fulton)\n",
    "print(model.score('County', ['Fulton']))\n",
    "\n",
    "# P(Grand | Fulton, County)\n",
    "print(model.score('Grand', ['Fulton', 'County']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0b521d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Canyon', 'voice', 'outstanding', 'above', 'the', 'year-earlier', 'level', '.', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# Now we have ngram model, let's use it to generate text\n",
    "# Generate 10 words after 'The'\n",
    "print(model.generate(10, text_seed=['Grand']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaa68b9",
   "metadata": {},
   "source": [
    "## POS tagging using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ea33f09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This  |  PRON  |  pronoun  |  DT  |  determiner\n",
      "is  |  AUX  |  auxiliary  |  VBZ  |  verb, 3rd person singular present\n",
      "the  |  DET  |  determiner  |  DT  |  determiner\n",
      "second  |  ADJ  |  adjective  |  JJ  |  adjective (English), other noun-modifier (Chinese)\n",
      "tutorial  |  NOUN  |  noun  |  NN  |  noun, singular or mass\n",
      "of  |  ADP  |  adposition  |  IN  |  conjunction, subordinating or preposition\n",
      "NLP  |  PROPN  |  proper noun  |  NNP  |  noun, proper singular\n",
      ".  |  PUNCT  |  punctuation  |  .  |  punctuation mark, sentence closer\n",
      "I  |  PRON  |  pronoun  |  PRP  |  pronoun, personal\n",
      "hope  |  VERB  |  verb  |  VBP  |  verb, non-3rd person singular present\n",
      "you  |  PRON  |  pronoun  |  PRP  |  pronoun, personal\n",
      "are  |  AUX  |  auxiliary  |  VBP  |  verb, non-3rd person singular present\n",
      "enjoying  |  VERB  |  verb  |  VBG  |  verb, gerund or present participle\n",
      "it  |  PRON  |  pronoun  |  PRP  |  pronoun, personal\n",
      ".  |  PUNCT  |  punctuation  |  .  |  punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"This is the second tutorial of NLP. I hope you are enjoying it.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,\" | \", token.pos_, \" | \", spacy.explain(token.pos_), \" | \", token.tag_, \" | \", spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eac1245",
   "metadata": {},
   "source": [
    "# Practice problems for Tierce-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9850005",
   "metadata": {},
   "source": [
    "## Autocomplete Problem using N gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "134e7945",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "model = MLE(N)\n",
    "train_data, padded_sents = padded_everygram_pipeline(N, full_brown)\n",
    "model.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d6742e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Autocomplete using ngram model\n",
    "# We will use the model trained on Brown corpus\n",
    "\n",
    "def auto_complete(previous_tokens, model, N):\n",
    "\n",
    "    # most recent 'n-1' words\n",
    "    prev_ngram = previous_tokens[max(-N+1, -len(previous_tokens)):]\n",
    "\n",
    "    max_prob_so_far = 0\n",
    "    best_word = None\n",
    "\n",
    "    for word in model.vocab:\n",
    "        if model.score(word, prev_ngram) > max_prob_so_far:\n",
    "            max_prob_so_far = model.score(word, prev_ngram)\n",
    "            best_word = word\n",
    "\n",
    "    return best_word\n",
    "\n",
    "# Let's try it out\n",
    "auto_complete(['first', 'time'], model, N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "11df50fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "election\n",
      "produced\n",
      "``\n",
      "no\n",
      "evidence\n",
      "of\n",
      "the\n",
      "United\n",
      "States\n",
      ",\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "my_tokens = ['recent', 'primary']\n",
    "for i in range(k):\n",
    "    next_word = auto_complete(my_tokens, model, N)\n",
    "    if next_word is not None:\n",
    "        print(next_word)\n",
    "        my_tokens.append(next_word)\n",
    "        my_tokens.pop(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60f706d",
   "metadata": {},
   "source": [
    "## Edit Distance Snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "585425e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def edit_distance_custom(word1, word2, backtrace=False, return_matrix=False):\n",
    "    m = len(word1)\n",
    "    n = len(word2)\n",
    "    dp = [[0 for i in range(n+1)] for j in range(m+1)]\n",
    "    for i in range(m+1):\n",
    "        for j in range(n+1):\n",
    "            if i == 0:\n",
    "                dp[i][j] = j\n",
    "            elif j == 0:\n",
    "                dp[i][j] = i\n",
    "            elif word1[i-1] == word2[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(dp[i][j-1], dp[i-1][j], dp[i-1][j-1])\n",
    "\n",
    "    if backtrace:\n",
    "        # backtracking\n",
    "        i = m\n",
    "        j = n\n",
    "        operations = []\n",
    "        while i > 0 and j > 0:\n",
    "            if word1[i-1] == word2[j-1]:\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "            elif dp[i][j] == dp[i-1][j-1] + 1:\n",
    "                operations.append(f\"Substitute {word1[i-1]} with {word2[j-1]}\")\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "            elif dp[i][j] == dp[i-1][j] + 1:\n",
    "                operations.append(f\"Delete {word1[i-1]}\")\n",
    "                i -= 1\n",
    "            elif dp[i][j] == dp[i][j-1] + 1:\n",
    "                operations.append(f\"Insert {word2[j-1]}\")\n",
    "                j -= 1\n",
    "        while i > 0:\n",
    "            operations.append(f\"Delete {word1[i-1]}\")\n",
    "            i -= 1\n",
    "        while j > 0:\n",
    "            operations.append(f\"Insert {word2[j-1]}\")\n",
    "            j -= 1\n",
    "\n",
    "        if return_matrix:\n",
    "            return dp[m][n], operations, dp\n",
    "        else:\n",
    "            return dp[m][n], operations\n",
    "\n",
    "    if return_matrix:\n",
    "        return dp[m][n], dp\n",
    "    else:\n",
    "        return dp[m][n]\n",
    "\n",
    "def edit_distance_with_transposition(word1, word2, backtrace=False, return_matrix=False):\n",
    "    m = len(word1)\n",
    "    n = len(word2)\n",
    "    dp = [[0 for i in range(n+1)] for j in range(m+1)]\n",
    "    for i in range(m+1):\n",
    "        for j in range(n+1):\n",
    "            if i == 0:\n",
    "                dp[i][j] = j\n",
    "            elif j == 0:\n",
    "                dp[i][j] = i\n",
    "            elif word1[i-1] == word2[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(dp[i][j-1], dp[i-1][j], dp[i-1][j-1])\n",
    "                if i > 1 and j > 1 and word1[i-1] == word2[j-2] and word1[i-2] == word2[j-1]:\n",
    "                    dp[i][j] = min(dp[i][j], dp[i-2][j-2] + 1)\n",
    "\n",
    "    if backtrace:\n",
    "        # backtracking\n",
    "        i = m\n",
    "        j = n\n",
    "        operations = []\n",
    "        while i > 0 and j > 0:\n",
    "            if word1[i-1] == word2[j-1]:\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "            elif dp[i][j] == dp[i-1][j-1] + 1:\n",
    "                operations.append(f\"Substitute {word1[i-1]} with {word2[j-1]}\")\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "            elif dp[i][j] == dp[i-1][j] + 1:\n",
    "                operations.append(f\"Delete {word1[i-1]}\")\n",
    "                i -= 1\n",
    "            elif dp[i][j] == dp[i][j-1] + 1:\n",
    "                operations.append(f\"Insert {word2[j-1]}\")\n",
    "                j -= 1\n",
    "            elif dp[i][j] == dp[i-2][j-2] + 1:\n",
    "                operations.append(f\"Transpose {word1[i-2]} and {word1[i-1]}\")\n",
    "                i -= 2\n",
    "                j -= 2\n",
    "        while i > 0:\n",
    "            operations.append(f\"Delete {word1[i-1]}\")\n",
    "            i -= 1\n",
    "        while j > 0:\n",
    "            operations.append(f\"Insert {word2[j-1]}\")\n",
    "            j -= 1\n",
    "\n",
    "        if return_matrix:\n",
    "            return dp[m][n], operations, dp\n",
    "        else:\n",
    "            return dp[m][n], operations\n",
    "    \n",
    "    if return_matrix:\n",
    "        return dp[m][n], dp\n",
    "    else:\n",
    "        return dp[m][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "66620ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# edit distance between two words\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "edit_distance(\"rain\", \"shine\", transpositions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e7fcd508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " ['Insert e', 'Substitute a with h', 'Substitute r with s'],\n",
       " [[0, 1, 2, 3, 4, 5],\n",
       "  [1, 1, 2, 3, 4, 5],\n",
       "  [2, 2, 2, 3, 4, 5],\n",
       "  [3, 3, 3, 2, 3, 4],\n",
       "  [4, 4, 4, 3, 2, 3]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance_custom(\"rain\", \"shine\", backtrace=True, return_matrix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d504c7bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " ['Substitute l with r', 'Substitute e with u', 'Transpose a and e'],\n",
       " [[0, 1, 2, 3, 4, 5, 6],\n",
       "  [1, 0, 1, 2, 3, 4, 5],\n",
       "  [2, 1, 1, 1, 2, 3, 4],\n",
       "  [3, 2, 1, 1, 2, 3, 4],\n",
       "  [4, 3, 2, 2, 1, 2, 3],\n",
       "  [5, 4, 3, 3, 2, 2, 3],\n",
       "  [6, 5, 4, 4, 3, 3, 3]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance_with_transposition(\"maezel\", \"meazur\", backtrace=True, return_matrix=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9665bf30",
   "metadata": {},
   "source": [
    "## Viterbi Distance Snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "104afc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viterbi decoding for POS tagging\n",
    "def viterbi_decoding(tokens, model, all_possible_tags):\n",
    "    # tokens: list of tokens\n",
    "    # model: ngram model\n",
    "    # returns: list of tags\n",
    "\n",
    "    # initialize\n",
    "    T = len(tokens)\n",
    "    # get all possible tags\n",
    "    tags = all_possible_tags\n",
    "\n",
    "    V = len(tags)\n",
    "    dp = [[0 for i in range(V)] for j in range(T)]\n",
    "    backpointers = [[0 for i in range(V)] for j in range(T)]\n",
    "\n",
    "    # base case\n",
    "    for i in range(V):\n",
    "        # dp[0][i] = model.score(tokens[0], tags[i])\n",
    "        if tokens[0] in model.keys() and tags[i] in model[tokens[0]].keys() and tags[i] in model.keys() and '<s>' in model[tags[i]].keys():\n",
    "            dp[0][i] = max(dp[0][i], model[tokens[0]][tags[i]] * model[tags[i]]['<s>'])\n",
    "\n",
    "        backpointers[0][i] = 0\n",
    "\n",
    "    # induction\n",
    "    for t in range(1, T):\n",
    "        # print('\\n' + '*'*50)\n",
    "        # print(f\"t = {t}\")\n",
    "        for i in range(V):\n",
    "            # print(f\"Current tag = {tags[i]}\")\n",
    "            max_prob_so_far = -1\n",
    "            best_tag = None\n",
    "            for j in range(V):\n",
    "                # print(f\"Previous tag = {tags[j]}\")\n",
    "                if tokens[t] in model.keys() and tags[i] in model[tokens[t]].keys() and tags[i] in model.keys() and tags[j] in model[tags[i]].keys():\n",
    "                    prob = dp[t-1][j] * model[tokens[t]][tags[i]] * model[tags[i]][tags[j]]\n",
    "                else:\n",
    "                    prob = -1\n",
    "\n",
    "                # print(f\"prob = {prob}\")\n",
    "\n",
    "                if prob > max_prob_so_far:\n",
    "                    max_prob_so_far = prob\n",
    "                    best_tag = j\n",
    "\n",
    "            dp[t][i] = max_prob_so_far\n",
    "            backpointers[t][i] = best_tag\n",
    "\n",
    "            # print(f\"dp[{t}][{i}] = {dp[t][i]}\")\n",
    "            # print(f\"backpointers[{t}][{i}] = {backpointers[t][i]}\")\n",
    "            # print()\n",
    "\n",
    "\n",
    "    # termination\n",
    "    max_prob_so_far = -1\n",
    "    best_tag = None\n",
    "    for i in range(V):\n",
    "        if dp[T-1][i] > max_prob_so_far:\n",
    "            max_prob_so_far = dp[T-1][i]\n",
    "            best_tag = i\n",
    "\n",
    "    # backtracking\n",
    "    best_sequence = [best_tag for i in range(T)]\n",
    "    for t in range(T-1, 0, -1):\n",
    "        best_sequence[t-1] = backpointers[t][best_sequence[t]]\n",
    "\n",
    "    return [tags[i] for i in best_sequence]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "cd9eb480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Noun', 'Verb', 'Verb']"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = {\n",
    "    'Det': {'<s>': 0.25}, \n",
    "    'Noun': {'<s>': 0.25, 'Det': 0.5, 'Noun': 0.2, 'Adj': 0.2, 'Verb': 0.3}, \n",
    "    'Adj': {'<s>': 0.25, 'Det': 0.3, 'Noun': 0.002}, \n",
    "    'Verb': {'<s>': 0.25, 'Det': 0.00001, 'Noun': 0.3, 'Adj': 0.001, 'Verb': 0.1},\n",
    "    'the': {'Det': 0.3, 'Noun': 0.1},\n",
    "    'light': {'Noun': 0.003, 'Adj': 0.002, 'Verb': 0.06},\n",
    "    'book': {'Noun': 0.003, 'Verb': 0.01},\n",
    "}\n",
    "\n",
    "tokens = ['the', 'light', 'book']\n",
    "tags = ['Det', 'Noun', 'Adj', 'Verb']\n",
    "\n",
    "viterbi_decoding(tokens, model, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7787ecb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3892fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af34b404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd002df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
